{"id":"29106606680","type":"IssueCommentEvent","actor":{"id":120408189,"login":"Larhzu","display_login":"Larhzu","gravatar_id":"","url":"https://api.github.com/users/Larhzu","avatar_url":"https://avatars.githubusercontent.com/u/120408189?"},"repo":{"id":553665726,"name":"tukaani-project/xz","url":"https://api.github.com/repos/tukaani-project/xz"},"payload":{"action":"created","issue":{"url":"https://api.github.com/repos/tukaani-project/xz/issues/50","repository_url":"https://api.github.com/repos/tukaani-project/xz","labels_url":"https://api.github.com/repos/tukaani-project/xz/issues/50/labels{/name}","comments_url":"https://api.github.com/repos/tukaani-project/xz/issues/50/comments","events_url":"https://api.github.com/repos/tukaani-project/xz/issues/50/events","html_url":"https://github.com/tukaani-project/xz/issues/50","id":1710506817,"node_id":"I_kwDOIQBEvs5l9ENB","number":50,"title":"[Feature Request]: A new filter for better compression of UTF-8 text (primarily Cyrillic, Greek or anything non-English)","user":{"login":"ssvb","id":78642,"node_id":"MDQ6VXNlcjc4NjQy","avatar_url":"https://avatars.githubusercontent.com/u/78642?v=4","gravatar_id":"","url":"https://api.github.com/users/ssvb","html_url":"https://github.com/ssvb","followers_url":"https://api.github.com/users/ssvb/followers","following_url":"https://api.github.com/users/ssvb/following{/other_user}","gists_url":"https://api.github.com/users/ssvb/gists{/gist_id}","starred_url":"https://api.github.com/users/ssvb/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ssvb/subscriptions","organizations_url":"https://api.github.com/users/ssvb/orgs","repos_url":"https://api.github.com/users/ssvb/repos","events_url":"https://api.github.com/users/ssvb/events{/privacy}","received_events_url":"https://api.github.com/users/ssvb/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-05-15T17:21:07Z","updated_at":"2023-05-16T16:05:47Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the Feature\r\n\r\nSimilar to BCJ filters for executables, UTF-8 text compression can be also potentially improved by applying a filter before compressing data with LZMA. Each Cyrillic letter needs 2 bytes in UTF-8 format. And it's easy to demonstrate that converting UTF-8 to legacy 8-bit encoding improves compression. Some examples are below.\r\n\r\nUkrainian text is **~7%** better compressible in a 8-bit cp1124 encoding:\r\n```\r\nwget https://archive.org/stream/ukrainskakuhnya1998/ukrainskakuhnya1998_djvu.txt\r\niconv -f utf-8 -t CP1124//TRANSLIT ukrainskakuhnya1998_djvu.txt > ukrainskakuhnya1998_djvu.txt.cp1124\r\nxz -k ukrainskakuhnya1998_djvu.txt\r\nxz -k ukrainskakuhnya1998_djvu.txt.cp1124\r\nls -l ukrainskakuhnya1998_djvu*\r\n-rw-r--r-- 1 ssvb ssvb 2860770 May 15 19:44 ukrainskakuhnya1998_djvu.txt\r\n-rw-r--r-- 1 ssvb ssvb 1682011 May 15 19:47 ukrainskakuhnya1998_djvu.txt.cp1124\r\n-rw-r--r-- 1 ssvb ssvb  418940 May 15 19:47 ukrainskakuhnya1998_djvu.txt.cp1124.xz\r\n-rw-r--r-- 1 ssvb ssvb  448944 May 15 19:44 ukrainskakuhnya1998_djvu.txt.xz\r\n```\r\n\r\nFinnish text is **~0.8%** better compressible in a 8-bit latin1 encoding:\r\n```\r\nwget https://www.gutenberg.org/cache/epub/70694/pg70694.html\r\niconv -f utf-8 -t latin1//TRANSLIT pg70694.html > pg70694.html.latin1\r\nxz -k pg70694.html\r\nxz -k pg70694.html.latin1\r\nls -l pg70694*\r\n-rw-r--r-- 1 ssvb ssvb 742324 May  4 03:00 pg70694.html\r\n-rw-r--r-- 1 ssvb ssvb 709415 May 15 19:16 pg70694.html.latin1\r\n-rw-r--r-- 1 ssvb ssvb 223924 May 15 19:16 pg70694.html.latin1.xz\r\n-rw-r--r-- 1 ssvb ssvb 225720 May  4 03:00 pg70694.html.xz\r\n```\r\n\r\nNow regarding the filter itself. It needs to be perfectly reversible without any possible data loss during conversion. So I would suggest to use a small set of legacy 8-bit codepages, such as latin1, cp1124, cp1251 and so on. But still reserve one special character for escape sequences (to switch between the available 8-bit codepages or for injecting arbitrary binary data). I can implement a prototype of such filter if anyone is interested. But the main question is how to integrate it into XZ? And whether such filter would be accepted in principle?\r\n\r\n### Expected Complications\r\n\r\nThat's a format extension. So older releases of `xz-utils` won't be able to decompress newly created `xz` files with this new filter.\r\n\r\n### Will I try to implement this new feature?\r\n\r\nYes","reactions":{"url":"https://api.github.com/repos/tukaani-project/xz/issues/50/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/tukaani-project/xz/issues/50/timeline","performed_via_github_app":null,"state_reason":null},"comment":{"url":"https://api.github.com/repos/tukaani-project/xz/issues/comments/1549960402","html_url":"https://github.com/tukaani-project/xz/issues/50#issuecomment-1549960402","issue_url":"https://api.github.com/repos/tukaani-project/xz/issues/50","id":1549960402,"node_id":"IC_kwDOIQBEvs5cYoTS","user":{"login":"Larhzu","id":120408189,"node_id":"U_kgDOBy1IfQ","avatar_url":"https://avatars.githubusercontent.com/u/120408189?v=4","gravatar_id":"","url":"https://api.github.com/users/Larhzu","html_url":"https://github.com/Larhzu","followers_url":"https://api.github.com/users/Larhzu/followers","following_url":"https://api.github.com/users/Larhzu/following{/other_user}","gists_url":"https://api.github.com/users/Larhzu/gists{/gist_id}","starred_url":"https://api.github.com/users/Larhzu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Larhzu/subscriptions","organizations_url":"https://api.github.com/users/Larhzu/orgs","repos_url":"https://api.github.com/users/Larhzu/repos","events_url":"https://api.github.com/users/Larhzu/events{/privacy}","received_events_url":"https://api.github.com/users/Larhzu/received_events","type":"User","site_admin":false},"created_at":"2023-05-16T16:05:47Z","updated_at":"2023-05-16T16:05:47Z","author_association":"MEMBER","body":"In principle, adding a filter for UTF-8 text files is fine. It just has to be done carefully as decoder support can never be removed (I don't want to end up with \"text filter\", \"a little better text filter\", \"hopefully the best text filter\"...).\r\n\r\nOld tools cannot decompress new filters but that's just how it is. Once a new filter is official, it takes some time until it can be used widely.\r\n\r\nYour example with ukrainskakuhnya1998_djvu.txt uses `CP1124//TRANSLIT` which means that it's not reversible: converting it back to UTF-8 gives 2838134 bytes, 0.8 % smaller than the original file. It still gives an indication how much a filter might help though.\r\n\r\nSimilarly, the pg70694.html isn't reversible and becomes 739319 bytes when restored to UTF-8, 0.4 % smaller than the original. I guess Finnish, Swedish, German, and such languages likely won't see big enough savings with a filter like this. The amount of non-ASCII characters is fairly low.\r\n\r\nUsing LZMA2 option `pb=0` tends to help with text files but with these files it makes no significant difference.\r\n\r\nConverting to UTF-16BE helps with ukrainskakuhnya1998_djvu.txt (it's 2-byte-aligned data thus `pb=1,lp=1`):\r\n\r\n```\r\n$ iconv -f utf8 -tutf16be < ukrainskakuhnya1998_djvu.txt > ukrainskakuhnya1998_djvu.txt.utf16be\r\n$ xz -k --lzma2=pb=1,lp=1 ukrainskakuhnya1998_djvu.txt.utf16be\r\n$ du -b --apparent ukrainskakuhnya1998_djvu.txt*\r\n2860770 ukrainskakuhnya1998_djvu.txt\r\n3343440 ukrainskakuhnya1998_djvu.txt.utf16be\r\n427884  ukrainskakuhnya1998_djvu.txt.utf16be.xz\r\n\r\n$ xz -k --lzma2=preset=6e,pb=1,lp=1 ukrainskakuhnya1998_djvu.txt.utf16be\r\n$ du -b --apparent ukrainskakuhnya1998_djvu.txt.utf16be.xz \r\n426312  ukrainskakuhnya1998_djvu.txt.utf16be.xz\r\n```\r\n\r\nSo it's not as good as your result but this is completely reversible as long as the original is valid UTF-8. Note that when compressing integers, big endian is usually better input than little endian.\r\n\r\nHave you looked at existing Unicode compression schemes like [SCSU](https://www.unicode.org/reports/tr6/tr6-4.html)? It may not be good here but perhaps some ideas can be had still, perhaps not.\r\n\r\nI understood that your idea could take a 8-bit codepage as an argument and then convert as much as possible using that, encoding unconvertible binary data via escape sequences or such. This could be fairly simple. On the other hand, it requires user to tell which codepage to use. In any case, character mapping should be done so that the decoder doesn't need to know any codepages: Filter Properties or the filtered raw stream itself should encode the mapping in some compact form instead.\r\n\r\nA more advanced idea could be to detect non-ASCII UTF-8 characters as they come in and assign a 8-bit replacement code for them. The advantage would be that then the filter would work with many languages without any configuration from the user. This is much more complex though. It should ensure that the same UTF-8 codepoint consistently gets mapped to the same 8-bit value, otherwise compression could be terrible. It matters if the input file has like 300 codepoints and thus not all of them can have an 8-bit mapping active at the same time. On the other hand, it's acceptable that compression ratio will be good only with certain languages.\r\n\r\nOn the second thought, perhaps the above is just too complicated. At least some of the languages (where this kind of filter could be useful) need one or perhaps two small contiguous codepoint ranges above ASCII. CP1124 is almost 0x0401-0x045F, only 0x0490-0x0491 are missing.\r\n\r\nMore random ideas: A two-byte UTF-8 sequence encodes 11-bit codepoint. It could be encoded as 0x10-0x17 followed by any 8-bit byte. Three-byte UTF-8 sequence encodes 16 bits so 0x18 followed by any two bytes would work (same length as in UTF-8). And four-byte UTF-8 could be 0x19 and three bytes. Then 0x80-0xFF could be used for language-specific 8-bit encodings and code points outside the language would still never use more space than in UTF-8 if the repurposed ASCII control codes aren't needed. A few more ASCII control codes would need to be repurposed for escaping binary data (including the repurposed control codes themselves) and possibly for configuring the 0x80-0xFF range (unless only using a static mapping from Filter Properties).\r\n\r\nA static mapping in Filter Properties could simply be a list of pairs <start><len>. For example, 0x0400 0x5F 0x0490 0x02 could put 0x0400-0x045F to 0x80-0xDF and 0x0490-0x0491 to 0xE0-0xE1, perhaps leaving 0xE2-0xFF to mean 0xE2-0xFF.\r\n\r\nThe above are just some quick ideas and better ones likely exist. :-)\r\n\r\nWhen deciding the encoded format of the filter, xz-file-format.txt section 5.2 must be taken into account. Basically, 200 bytes of input to a decoder must produce at least 100 bytes of output for security reasons.\r\n\r\nFor early prototypes, standalone filter programs that filter from stdin to stdout are probably the most convenient.\r\n\r\nIf you wish to add prototype filters in .xz, please use a custom filter as described in xz-file-format.txt section 5.4. The small ID numbers must be used for final official filters only.\r\n\r\nHow to add the actual code: Look how Delta filter hooks into the rest of the code. Delta filter is very simple and doesn't change the size of the data. A text filter would change the size of the data so it would likely need some internal buffering. XZ for Java has cleaner codebase than XZ Utils so, depending on your preferences, Java code might be nicer for prototyping than C in this case.\r\n\r\nWe can talk on IRC on #tukaani at Libera Chat too, if you wish (and we happen to be online at the same time).\r\n","reactions":{"url":"https://api.github.com/repos/tukaani-project/xz/issues/comments/1549960402/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null}},"public":true,"created_at":"2023-05-16T16:05:47Z","org":{"id":116083088,"login":"tukaani-project","gravatar_id":"","url":"https://api.github.com/orgs/tukaani-project","avatar_url":"https://avatars.githubusercontent.com/u/116083088?"}}
